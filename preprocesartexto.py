import os
import string
# import fontstyle
# import tkinter
import unicodedata
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from dotenv import load_dotenv
import pandas as pd
import plotly.graph_objects as go

nltk.download('punkt')
nltk.download('stopwords')

stopWords = set(stopwords.words('spanish'))

# Ajustar las opciones de visualizaciÃ³n en terminal
pd.set_option('display.max_rows', None)  # Muestra todas las filas
pd.set_option('display.max_columns', None)  # Muestra todas las columnas
pd.set_option('display.width', None)  # Ajusta el ancho de la visualizaciÃ³n
pd.set_option('display.max_colwidth', None)  # Muestra el contenido completo de las columnas

# Cargar variables de entorno
load_dotenv()
PATH_SUBJECTSTRAIN = os.getenv('PATH_SUBJECTSTRAIN')
PATH_TRAIN = os.getenv('PATH_TRAIN')
PATH_CSVLINKEDFILES = os.getenv('PATH_CSVLINKEDFILES')

# Declarar de dict para guardar los subjects preprocesados
dataArray = {
    'Subject': [],
    'id_message': [],
    'message': [],
    'preproccedMessage': [],
    'date': [],
    'label': []
}

# Declarar array para contar las palabras
wordCount = {}

# Declarar array de stop words no contenidas en nltk
stop_words = stopWords.union(
    ["a", "acÃ¡", "ahÃ­", "ajena", "ajeno", "ajenos", "ajenas", "al", "algo", "algÃºn", "alguna", "alguns", "algunos",
     "allÃ¡", "allÃ­", "ambos", "ante", "antes", "aquel", "aquella", "aquello", "aquÃ­", "arriba", "asÃ­", "atrÃ¡s", "aun",
     "aunque", "bajo", "bastante", "bien", "cabe", "cada", "casi", "cierto", "cierta", "ciertos", "ciertas",
     "como", "con", "conmigo", "conseguimos", "conseguir", "consigo", "consigue", "consiguen", "consigues",
     "contigo", "contra", "cual", "cuales", "cualquier", "cualquiera", "cualquieras", "cuan", "cuando",
     "cuanto", "cuanta", "cuantos", "cuantas", "de", "dejar", "del", "demÃ¡s", "demasiado", "demasiada", "demasiados",
     "demasiadas", "dentro", "desde", "donde", "dos", "el", "Ã©l", "ella", "ellas", "ellos", "empleÃ¡is", "emplean",
     "emplear", "empleas", "empleo", "en", "encima", "entonces", "entre", "era", "eras", "eramos", "eran", "eres",
     "es", "esa", "ese", "eso", "esas", "esos", "esta", "estas", "estaba", "estado", "estÃ¡is", "estamos", "estÃ¡n",
     "estar", "este", "esto", "estoy", "etc", "fin", "fue", "fueron", "fui", "fuimos", "gueno", "ha", "hace", "haces",
     "hacÃ©is", "hacemos", "hacen", "hacer", "hacia", "hago", "hasta", "incluso", "intenta", "intentas", "intentÃ¡is",
     "intentamos", "intentan", "intentar", "intento", "ir", "jamÃ¡s", "junto", "juntos", "la", "las", "lo", "los",
     "largo", "mÃ¡s", "me", "menos", "mi", "mÃ­a", "mÃ­as", "mÃ­o", "mÃ­os", "mis", "misma", "mismo", "mismas", "modos",
     "mucha", "muchas", "muchÃ­simo", "muchÃ­sima", "muchÃ­simos", "muchÃ­simas", "mucho", "muchos", "muy", "nada", "ni",
     "ningÃºn", "ninguna", "ninguno", "ningunas", "no", "nos", "nosotras", "nosotros", "nuestra", "nuestro", "nuestras",
     "nuestros", "nunca", "os", "otra", "otro", "otras", "otros", "para", "parecer", "pero", "poca", "poco", "pocas",
     "podÃ©is", "podemos", "poder", "podrÃ­a", "podrÃ­as", "podrÃ­ais", "podrÃ­amos", "podrÃ­an", "por", "por quÃ©", "porque",
     "primero", "puede", "pueden", "puedo", "pues", "que", "quÃ©", "querer", "quiÃ©n", "quiÃ©nes", "quienesquiera",
     "quienquiera", "quizÃ¡", "quizÃ¡s", "sabe", "sabes", "saben", "sabÃ©is", "sabemos", "saber", "se", "segÃºn", "ser",
     "si", "sÃ­", "siempre", "siendo", "sin", "sino", "so", "sobre", "sois", "solamente", "solo", "sÃ³lo", "somos",
     "soy", "sr", "sra", "sres", "sta", "su", "suya", "suyo", "suyas", "suyos", "tal", "tales", "tambiÃ©n", "tampoco",
     "tan", "tanta", "tantas", "tanto", "tantos", "te", "tenÃ©is", "tenemos", "tener", "tengo", "ti", "tiempo",
     "tiene", "tienen", "toda", "todo", "todas", "todos", "tomar", "trabaja", "trabajo", "trabajÃ¡is", "trabajamos",
     "trabajan", "trabajar", "trabajas", "tras", "tÃº", "tu", "tus", "tuya", "tuyo", "tuyas", "tuyos", "Ãºltimo",
     "ultimo", "un", "una", "uno", "unas", "unos", "usa", "usas", "usÃ¡is", "usamos", "usan", "usar", "uso", "usted",
     "ustedes", "va", "van", "vais", "valor", "vamos", "varias", "varios", "vaya", "verdadera", "vosotras", "vosotros",
     "voy", "vuestra", "vuestro", "vuestras", "vuestros", "y", "ya", "yo"])

# Declarar array de signos de puntuacion
puntuacion = ['___', 'âœ¦', '...', 'â€œ', 'Â«', 'âœ—', 'Â¿', 'Â»', 'â£', '``', 'Â°', 'â”']

# Declarar diccionario de emojis
emojis = {
    "ğŸ™‹â€â™€": "mujer con la mano levantada",
    "ğŸƒğŸ¾â€â™€": "mujer corriendo: tono de piel oscuro medio",
    "ğŸ¤·â€â™€": "mujer encogida de hombros",
    "ğŸ«¡": "hola",
    "ğŸ‹â€â™‚": "persona levantando pesas signo masculino",
    "ğŸ‡´": "la letra o mayuscula cirilo",
    "ğŸ’†â€â™€": "mujer recibiendo masaje",
    "ğŸ™†â€â™€": "mujer haciendo el gesto de de acuerdoï¸",
    "2âƒ£": "teclas 2",
    "ğŸ¤¦ğŸ»â€â™€": "mujer con la mano en la frente tono de piel claro",
    "ğŸ¤¦â€â™€": "mujer con la mano en la frente",
    "ğŸ« ": "estoy derretido de felicidad",
    "ğŸ‡¨": "la letra c",
    "ğŸ™‹ğŸ»â€â™€": "mujer con la mano levantada tono de piel claro",
    "ğŸ™‹ğŸ½â€â™€": "mujer con la mano levantada tono de piel medio",
    "ğŸ¤¸ğŸ»â€â™€": "mujer haciendo voltereta lateral tono de piel claro",
    "ğŸ§šâ€â™€": "hada hembra",
    "ğŸ‘¨â€âš•": "profesional sanitario hombre",
    "1âƒ£": "teclas 1",
    "ğŸ¤¦â€â™‚": "hombre con la mano en la frente",
    "ğŸ¤¦ğŸ¼â€â™€": "mujer con la mano en la frente tono de piel claro medio",
    "ğŸ¤·ğŸ»â€â™€": "mujer encogida de hombros tono de piel claro",
    "ğŸ™‹ğŸ¼â€â™€": "mujer con la mano levantada tono de piel claro medio",
    "3âƒ£": "teclas 3"
}

# Declarar diccionario para corregir las palabras con una fuente diferente

fontsWords = {
# Acentos


# Primer set
    "á´€": 'A',
    "á´…": 'D',
    "á´‡": 'E',
    "Ò“": 'F',
    "Éª": 'I',
    "á´": 'M',
    "É´": 'N',
    "á´": 'O',
    "Ê€": 'R',
    "s": 'S',
    "á´›": 'T',
    "á´ ": 'V',
    "Ê": 'Y',
# Segundo set
    "ğ´": 'A',
    "ğ‘": 'a',
    "ğ‘Ì€": 'a',
    "ğ‘": 'b',
    "ğ¶": 'C',
    "ğ‘": 'c',
    "ğ·": 'D',
    "ğ‘‘": 'd',
    "ğ¸": 'E',
    "ğ‘’": 'e',
    "ğ‘“": 'f',
    "ğ‘”": 'g',
    "ğ»": 'H',
    "â„": 'h',
    "ğ‘–": 'i',
    "ğ‘—": 'j',
    "ğ‘˜": 'k',
    "ğ¿": 'L',
    "ğ‘™": 'l',
    "ğ‘€": 'M',
    "ğ‘š": 'm',
    "ğ‘": 'N',
    "ğ‘›": 'n',
    "ğ‘œ": 'o',
    "ğ‘œÌ": 'o',
    "ğ‘ƒ": 'P',
    "ğ‘": 'p',
    "ğ‘„": 'Q',
    "ğ‘": 'q',
    "ğ‘…": 'R',
    "ğ‘Ÿ": 'r',
    "ğ‘†": 'S',
    "ğ‘ ": 's',
    "ğ‘‡": 'T',
    "ğ‘¡": 't',
    "ğ‘ˆ": 'U',
    "ğ‘¢": 'u',
    "ğ‘£": 'v',
    "ğ‘¥": 'x',
    "ğ‘¦": 'y',
    "ğ‘§": 'z',
# Tercer set
    "ğ€": 'A',
    "ğ": 'B',
    "ğ„": 'E',
    "ğ‡": 'H',
    "ğˆ": 'I',
    "ğ‹": 'L',
    "ğŒ": 'M',
    "ğ": 'N',
    "ğ": 'O',
    "ğ": 'P',
    "ğ‘": 'R',
    "ğ’": 'S',
    "ğ˜": 'Y',

    "": 'A',
    "": 'a',
    "": 'B',
    "": 'b',
    "": 'C',
    "": 'c',
    "": 'D',
    "": 'd',
    "": 'E',
    "": 'e',
    "": 'F',
    "": 'f',
    "": 'G',
    "": 'g',
    "": 'H',
    "": 'h',
    "": 'I',
    "": 'i',
    "": 'J',
    "": 'j',
    "": 'K',
    "": 'k',
    "": 'L',
    "": 'l',
    "": 'M',
    "": 'm',
    "": 'N',
    "": 'n',
    "": 'Ã‘',
    "": 'Ã±',
    "": 'O',
    "": 'o',
    "": 'P',
    "": 'p',
    "": 'Q',
    "": 'q',
    "": 'R',
    "": 'r',
    "": 'S',
    "": 's',
    "": 'T',
    "": 't',
    "": 'U',
    "": 'u',
    "": 'V',
    "": 'v',
    "": 'W',
    "": 'w',
    "": 'X',
    "": 'x',
    "": 'Y',
    "": 'y',
    "": 'Z',
    "": 'z',
}
wordStrange = {}


def readJSONFiles():
    # Obtener el nombre del archivo para cada archivo en la ruta TRAIN
    for name_file in os.listdir(PATH_SUBJECTSTRAIN):
        # leer JSON
        file = pd.read_json(f"{PATH_SUBJECTSTRAIN}/{name_file}")
        preprocesstext(file, name_file)


# FunciÃ³n para leer los archivos JSON y crear un DataFrame con el nombre del archivo y
# los datos asociados

# FunciÃ³n para leer las etiquetas
def readTXT():
    trainLabel = pd.read_csv(PATH_TRAIN)
    # print(trainLabel[trainLabel['label']==1])
    # print(trainLabel)
    return trainLabel

# Preprocesar texto
def preprocesstext(file, name_file):
    trainLabel = readTXT()
    i = 0
    # print(f"Preprocessado del {name_file}")

    # Agregar la columna subject
    file.insert(0, 'Subject', name_file)
    # Agregar la columna para el texto preprocesado
    file.insert(3, 'preproccedMessage', " ")

    # Obtener el label para un subject especifico
    specLabel = trainLabel[trainLabel['Subject'] == name_file.replace('.json', '')]
    # Agregar la columna label e insertar el valor correspondiente
    file.insert(5, 'label', specLabel['label'].values[0])

    for content in file['message']:
        # Convertir las palabras en la fuente general
        tempMessage = ''.join(fontsWords[caracter] if caracter in fontsWords else caracter for caracter in content)

        # Cambiar emoji por texto
        tempMessage = ''.join(emojis[caracter] if caracter in emojis else caracter for caracter in tempMessage)

        # Eliminar signos de puntuacion extraÃ±os, cambiar letra acentuada
        tempMessage = unicodedata.normalize('NFKD', tempMessage).encode('ASCII', 'ignore').decode('utf-8')
        tempMessage = tempMessage.replace('.',' ').replace('-',' ')

        # Tokenizar y convertir a minuscula
        tempMessage = word_tokenize(tempMessage.lower())

        # Eliminar stop words
        tempMessage = [tempMessage for tempMessage in tempMessage if tempMessage.lower() not in stop_words]

        # Eliminar signos de puntuacion
        tempMessage = [tempMessage for tempMessage in tempMessage if tempMessage not in string.punctuation]
        tempMessage = [tempMessage for tempMessage in tempMessage if tempMessage not in puntuacion]

        for word in tempMessage:
            if word in wordCount:
                wordCount[word] = wordCount[word] + 1
                wordStrange[word] = name_file
            else:
                wordCount[word] = 1
                wordStrange[word] = name_file

        # Regresar tempMessage a una oracion
        tempMessage = ' '.join(tempMessage)
        # Agregar en la columna preproccedMessage el texto preprocesado
        file.loc[i, 'preproccedMessage'] = tempMessage
        i = i + 1

    # Agregar al dict dataArray los subjects
    dataArray['Subject'].append(name_file.replace('.json', ''))
    dataArray['id_message'].append(file['id_message'])
    dataArray['message'].append(file['message'])
    dataArray['preproccedMessage'].append(file['preproccedMessage'])
    dataArray['date'].append(file['date'])
    dataArray['label'].append(file['label'])

    # TODO agregar signos que no estÃ¡n es string.punctuation


readJSONFiles()

# Convertir a Data Frame y guardar como JSON
finalFile = pd.DataFrame(dataArray.items())
finalFile.to_json(f'{PATH_CSVLINKEDFILES}ab.json')

# Ordenar de mayor a menor las palabras
wordCount = dict(sorted(wordCount.items(), key=lambda item: item[1], reverse=True))
word = list(wordCount.keys())
count = list(wordCount.values())

# Crear una tabla de las palabras tokenizadas y su cantidad
wordCountTab = go.Figure(data=[go.Table(header=dict(values=['Word', 'Count']),
                                        cells=dict(values=[word, count])
                                        )])
# Mostrar tabla
wordCountTab.show()

df = pd.DataFrame(wordCount.items(), columns=['Palabra','Cantidad'])
df.to_csv(f'{PATH_CSVLINKEDFILES}new.csv')