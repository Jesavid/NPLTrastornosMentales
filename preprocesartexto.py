import os
import string
import re
import unicodedata
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from dotenv import load_dotenv
import pandas as pd
import plotly.graph_objects as go


nltk.download('punkt')
nltk.download('stopwords')

stopWords = set(stopwords.words('spanish'))
stopWordsEn = set(stopwords.words('english'))

# Ajustar las opciones de visualizaciÃ³n en terminal
pd.set_option('display.max_rows', None)  # Muestra todas las filas
pd.set_option('display.max_columns', None)  # Muestra todas las columnas
pd.set_option('display.width', None)  # Ajusta el ancho de la visualizaciÃ³n
pd.set_option('display.max_colwidth', None)  # Muestra el contenido completo de las columnas

# Cargar variables de entorno
load_dotenv()

PATH_SUBJECTSTRAIN = os.getenv('PATH_SUBJECTSTRAIN')
PATH_TRAIN = os.getenv('PATH_TRAIN')

PATH_SUBJECTSTRIAL = os.getenv('PATH_SUBJECTSTRIAL')
PATH_TRIAL = os.getenv('PATH_TRIAL')

paths = {
    "type": ['train', 'trial'],
    "subjects": [PATH_SUBJECTSTRAIN, PATH_SUBJECTSTRIAL],
    "label": [PATH_TRAIN, PATH_TRIAL]
}

PATH_FINALFILE = os.getenv('PATH_FINALFILE')

# Declarar dict para guardar los subjects preprocesados
dataArray = {
    'Subject': [],
    'id_message': [],
    'message': [],
    'preproccedMessage': [],
    'date': [],
    'label': []
}

# Declarar dict para guardar subject, message, label
corpus = {
    'subject': [],
    'message': [],
    'label': []
}

# Declarar dict para contar las palabras
wordCount = {}

# Declarar dict para ver las palabras y su subject asociado
wordStrange = {}

# Declarar array de stop words no contenidas en nltk
stop_words = stopWordsEn.union(stopWords.union(
    [
        "a", "al", "algo", "algunas", "algunos", "ante", "antes", "como", "con", "contra", "cual", "cuando", "de",
        "del", "desde",
        "donde", "durante", "e", "el", "ella", "ellas", "ellos", "en", "entre", "era", "erais", "Ã©ramos", "eran",
        "eras", "eres",
        "es", "esa", "esas", "ese", "esos", "esta", "estaba", "estabais", "estÃ¡bamos", "estaban", "estabas", "estad",
        "estada",
        "estadas", "estado", "estados", "estamos", "estando", "estar", "estaremos", "estarÃ¡", "estarÃ¡n", "estarÃ¡s",
        "estarÃ©",
        "estarÃ©is", "estarÃ­a", "estarÃ­ais", "estarÃ­amos", "estarÃ­an", "estarÃ­as", "estas", "este", "estemos", "esto",
        "estos",
        "estoy", "estuve", "estuviera", "estuvierais", "estuviÃ©ramos", "estuvieran", "estuvieras", "estuvieron",
        "estuviese",
        "estuvieseis", "estuviÃ©semos", "estuviesen", "estuvieses", "estuvimos", "estuviste", "estuvisteis",
        "estuviÃ©ramos",
        "estuviÃ©semos", "estuvo", "ex", "excepto", "fue", "fuera", "fuerais", "fuÃ©ramos", "fueran", "fueras", "fueron",
        "fuese",
        "fueseis", "fuÃ©semos", "fuesen", "fueses", "fui", "fuimos", "fuiste", "fuisteis", "ha", "habÃ­a", "habÃ­ais",
        "habÃ­amos",
        "habÃ­an", "habÃ­as", "habÃ©is", "habida", "habidas", "habido", "habidos", "habiendo", "habrÃ¡", "habrÃ¡n", "habrÃ¡s",
        "habrÃ©",
        "habrÃ©is", "habremos", "habrÃ­a", "habrÃ­ais", "habrÃ­amos", "habrÃ­an", "habrÃ­as", "habÃ©is", "habÃ­a", "habÃ­ais",
        "habÃ­amos",
        "habÃ­an", "habÃ­as", "hace", "haceis", "hacemos", "hacen", "hacer", "hacerlo", "haces", "hacia", "haciendo",
        "hago",
        "han", "has", "hasta", "hay", "haya", "hayamos", "hayan", "hayas", "he", "hecho", "hemos", "hube", "hubiera",
        "hubierais",
        "hubiÃ©ramos", "hubieran", "hubieras", "hubieron", "hubiese", "hubieseis", "hubiÃ©semos", "hubiesen", "hubieses",
        "hubimos",
        "hubiste", "hubisteis", "hubiÃ©ramos", "hubiÃ©semos", "hubo", "la", "las", "le", "les", "lo", "los", "me", "mi",
        "mis",
        "mucho", "muchos", "muy", "mÃ¡s", "mÃ­", "mÃ­a", "mÃ­as", "mÃ­o", "mÃ­os", "nada", "ni", "ningÃºn", "ninguna",
        "ningunas",
        "ninguno", "ningunos", "no", "nos", "nosotras", "nosotros", "nuestra", "nuestras", "nuestro", "nuestros",
        "nunca",
        "os", "otra", "otras", "otro", "otros", "para", "parecer", "pero", "poca", "pocas", "poco", "pocos", "podÃ©is",
        "podemos",
        "poder", "podrÃ­a", "podrÃ­amos", "podrÃ­an", "podrÃ­as", "poner", "por", "porque", "primero", "puede", "pueden",
        "puedo",
        "pues", "que", "quÃ©", "querer", "quiÃ©n", "quienes", "quiere", "quiÃ©nes", "quiso", "saber", "se", "sÃ©", "ser",
        "si",
        "sÃ­", "siendo", "sin", "sino", "so", "sobre", "sois", "solamente", "solo", "sÃ³lo", "somos", "son", "soy", "su",
        "sus", "suya", "suyas", "suyo", "suyos", "sÃ­", "tambiÃ©n", "tanto", "te", "tendrÃ©", "tendrÃ©is", "tendremos",
        "tendrÃ­a",
        "tendrÃ­ais", "tendrÃ­amos", "tendrÃ­an", "tendrÃ­as", "tened", "tenemos", "tener", "tenga", "tengamos", "tengan",
        "tengas", "tengo", "tenÃ­a", "tenÃ­ais", "tenÃ­amos", "tenÃ­an", "tenÃ­as", "ti", "tiene", "tienen", "tienes",
        "todo",
        "todos", "tu", "tus", "tuve", "tuviera", "tuvierais", "tuviÃ©ramos", "tuvieran", "tuvieras", "tuvieron",
        "tuviese",
        "tuvieseis", "tuviÃ©semos", "tuviesen", "tuvieses", "tuvimos", "tuviste", "tuvisteis", "tuviÃ©ramos",
        "tuviÃ©semos",
        "tuvo", "tuya", "tuyas", "tuyo", "tuyos", "un", "una", "uno", "unos", "usa", "usamos", "usan", "usar", "usas",
        "uso", "usted", "ustedes", "va", "vais", "valor", "vamos", "van", "varias", "varios", "vaya", "veces", "ver",
        "verdad", "verdadera", "verdadero", "vosotras", "vosotros", "voy", "vuestra", "vuestras", "vuestro", "vuestros",
        "y", "ya", "yo", "Ã©l", "Ã©ramos", "Ã©sa", "Ã©sas", "Ã©se", "Ã©sos", "Ã©sta", "Ã©stas", "Ã©ste", "Ã©stos", "Ãºltima",
        "Ãºltimas",
        "Ãºltimo", "Ãºltimos"
    ]
))

# Declarar array de signos de puntuacion
puntuacion = ['___', 'âœ¦', '...', 'â€œ', 'Â«', 'âœ—', 'Â¿', 'Â»', 'â£', '``', 'Â°', 'â”']

# Declarar diccionario de emojis
emojis = {
    "ğŸ™‹â€â™€": "mujer con la mano levantada",
    "ğŸƒğŸ¾â€â™€": "mujer corriendo: tono de piel oscuro medio",
    "ğŸ¤·â€â™€": "mujer encogida de hombros",
    "ğŸ«¡": "hola",
    "ğŸ‹â€â™‚": "persona levantando pesas signo masculino",
    "ğŸ‡´": "la letra o mayuscula cirilo",
    "ğŸ’†â€â™€": "mujer recibiendo masaje",
    "ğŸ™†â€â™€": "mujer haciendo el gesto de de acuerdoï¸",
    "2âƒ£": "teclas 2",
    "ğŸ¤¦ğŸ»â€â™€": "mujer con la mano en la frente tono de piel claro",
    "ğŸ¤¦â€â™€": "mujer con la mano en la frente",
    "ğŸ« ": "estoy derretido de felicidad",
    "ğŸ‡¨": "la letra c",
    "ğŸ™‹ğŸ»â€â™€": "mujer con la mano levantada tono de piel claro",
    "ğŸ™‹ğŸ½â€â™€": "mujer con la mano levantada tono de piel medio",
    "ğŸ¤¸ğŸ»â€â™€": "mujer haciendo voltereta lateral tono de piel claro",
    "ğŸ§šâ€â™€": "hada hembra",
    "ğŸ‘¨â€âš•": "profesional sanitario hombre",
    "1âƒ£": "teclas 1",
    "ğŸ¤¦â€â™‚": "hombre con la mano en la frente",
    "ğŸ¤¦ğŸ¼â€â™€": "mujer con la mano en la frente tono de piel claro medio",
    "ğŸ¤·ğŸ»â€â™€": "mujer encogida de hombros tono de piel claro",
    "ğŸ™‹ğŸ¼â€â™€": "mujer con la mano levantada tono de piel claro medio",
    "3âƒ£": "teclas 3"
}

# Declarar diccionario para corregir las palabras con una fuente diferente
fontsWords = {
    # Primer set
    "á´€": 'A',
    "á´…": 'D',
    "á´‡": 'E',
    "Ò“": 'F',
    "Éª": 'I',
    "á´": 'M',
    "É´": 'N',
    "á´": 'O',
    "Ê€": 'R',
    "s": 'S',
    "á´›": 'T',
    "á´ ": 'V',
    "Ê": 'Y',
    # Segundo set
    "ğ´": 'A',
    "ğ‘": 'a',
    "ğ‘Ì€": 'a',
    "ğ‘": 'b',
    "ğ¶": 'C',
    "ğ‘": 'c',
    "ğ·": 'D',
    "ğ‘‘": 'd',
    "ğ¸": 'E',
    "ğ‘’": 'e',
    "ğ‘“": 'f',
    "ğ‘”": 'g',
    "ğ»": 'H',
    "â„": 'h',
    "ğ‘–": 'i',
    "ğ‘—": 'j',
    "ğ‘˜": 'k',
    "ğ¿": 'L',
    "ğ‘™": 'l',
    "ğ‘€": 'M',
    "ğ‘š": 'm',
    "ğ‘": 'N',
    "ğ‘›": 'n',
    "ğ‘œ": 'o',
    "ğ‘œÌ": 'o',
    "ğ‘ƒ": 'P',
    "ğ‘": 'p',
    "ğ‘„": 'Q',
    "ğ‘": 'q',
    "ğ‘…": 'R',
    "ğ‘Ÿ": 'r',
    "ğ‘†": 'S',
    "ğ‘ ": 's',
    "ğ‘‡": 'T',
    "ğ‘¡": 't',
    "ğ‘ˆ": 'U',
    "ğ‘¢": 'u',
    "ğ‘£": 'v',
    "ğ‘¥": 'x',
    "ğ‘¦": 'y',
    "ğ‘§": 'z',
    # Tercer set
    "ğ€": 'A',
    "ğ": 'B',
    "ğ„": 'E',
    "ğ‡": 'H',
    "ğˆ": 'I',
    "ğ‹": 'L',
    "ğŒ": 'M',
    "ğ": 'N',
    "ğ": 'O',
    "ğ": 'P',
    "ğ‘": 'R',
    "ğ’": 'S',
    "ğ˜": 'Y'
}

# Declarar patron de expreciones regulares
patron = r'\b(ah+ah+|aja+|aj+aj+|ja+ja|ja+|ha+|he+|ah+|aja|aja+j+|je+|j+e|je+je+|ja+j+a+|\d[a-zA-Z]\d)\b'

def readJSONFiles(index):
    # Obtener el nombre del archivo para cada archivo en la ruta
    for name_file in os.listdir(paths['subjects'][index]):
        # leer JSON
        file = pd.read_json(f"{paths['subjects'][index]}/{name_file}")
        preprocesstext(file, name_file)


# FunciÃ³n para leer las etiquetas
def readTXT(index):
    trainLabel = pd.read_csv(paths['label'][index])
    return trainLabel


# Preprocesar texto
def preprocesstext(file, name_file):
    trainLabel = readTXT(index)
    i = 0
    concatMessage = ""

    # Agregar la columna subject
    file.insert(0, 'Subject', name_file)
    # Agregar la columna para el texto preprocesado
    file.insert(3, 'preproccedMessage', " ")

    # Obtener el label para un subject especifico
    specLabel = trainLabel[trainLabel['Subject'] == name_file.replace('.json', '')]
    # Agregar la columna label e insertar el valor correspondiente
    file.insert(5, 'label', specLabel['label'].values[0])

    # Leer cada mensaje de cada subject y preprocesar el texto
    for content in file['message']:
        # Convertir las palabras en la fuente general
        tempMessage = ''.join(fontsWords[caracter] if caracter in fontsWords else caracter for caracter in content)

        # Cambiar emoji por texto
        tempMessage = ''.join(emojis[caracter] if caracter in emojis else caracter for caracter in tempMessage)

        # Eliminar palabras con expreciones regulares
        tempMessage = re.sub(patron, '', tempMessage, flags=re.IGNORECASE)

        # Eliminar signos de puntuacion extraÃ±os, cambiar letra acentuada
        tempMessage = unicodedata.normalize('NFKD', tempMessage).encode('ASCII', 'ignore').decode('utf-8')
        tempMessage = tempMessage.replace('.', ' ').replace('-', ' ').replace('___', " ")

        # Tokenizar y convertir a minuscula
        tempMessage = word_tokenize(tempMessage.lower())

        # Eliminar stop words
        tempMessage = [tempMessage for tempMessage in tempMessage if tempMessage.lower() not in stop_words]

        # Eliminar signos de puntuacion
        tempMessage = [tempMessage for tempMessage in tempMessage if tempMessage not in string.punctuation]
        tempMessage = [tempMessage for tempMessage in tempMessage if tempMessage not in puntuacion]

        # Contar el total de las palabras de todos los subjects
        for word in tempMessage:
            if word in wordCount:
                wordCount[word] = wordCount[word] + 1
                wordStrange[word] = name_file
            else:
                wordCount[word] = 1
                wordStrange[word] = name_file

        # Regresar tempMessage a una oracion
        tempMessage = ' '.join(tempMessage)
        concatMessage += tempMessage + " "
        # Agregar en la columna preproccedMessage el texto preprocesado
        file.loc[i, 'preproccedMessage'] = tempMessage
        i = i + 1

    # Agregar al dict dataArray los subjects
    dataArray['Subject'].append(name_file.replace('.json', ''))
    dataArray['id_message'].append(file['id_message'])
    dataArray['message'].append(file['message'])
    dataArray['preproccedMessage'].append(file['preproccedMessage'])
    dataArray['date'].append(file['date'])
    dataArray['label'].append(file['label'])

    # Agregar datos al corpus
    corpus['subject'].append(name_file.replace('.json', ''))
    corpus['message'].append(concatMessage)
    corpus['label'].append(file['label'][0])


for index in range(len(next(iter(paths.values())))):
    readJSONFiles(index)

    # Convertir a Data Frame y guardar como JSON
    finalFile = pd.DataFrame(dataArray.items())
    finalFile.to_json(f'{PATH_FINALFILE}{paths['type'][index]}.json')

    # Crear Data Frame y guardar CSV de las palabras y su cantidad
    # df = pd.DataFrame(wordCount.items(), columns=['Palabra', 'Cantidad'])
    # df.to_csv(f'{PATH_FINALFILE}{paths['type'][index]}.csv')

    # Ordenar de mayor a menor las palabras
    # wordCount = dict(sorted(wordCount.items(), key=lambda item: item[1], reverse=True))
    # word = list(wordCount.keys())
    # count = list(wordCount.values())

    # Crear una tabla de las palabras tokenizadas y su cantidad
    # wordCountTab = go.Figure(data=[go.Table(header=dict(values=['Word', 'Count']),
    #                                         cells=dict(values=[word, count])
    #                                         )])
    # Mostrar tabla
    # wordCountTab.show()

    #Convertir a DF y guardar corpus com JSON
    df = pd.DataFrame(corpus)
    df.to_json(f'{PATH_FINALFILE}{paths['type'][index]}Corpus.json')

    # Vaciar diccionario
    for key in dataArray:
        dataArray[key].clear()
    wordCount.clear()
    for key in corpus:
        corpus[key].clear()